# 读书笔记

## 第四章 训练模型

### 线性回归

概括的讲，线性模型就是对输入特征加权求和，再加上一个称之为偏置项（或截距项）的常数，并以此进行预测，如下公式所示：
$$
y = \theta_0 + \theta_1x_1 + \theta_2x_2 + ...  + \theta_nx_n
$$
在此公式中：
* `y`是预测值
* `n`是特征数量
* $x_i$是第`i`个特征值
* $\theta_j$是第`j`个模型参数（包括偏差项$\theta_0$和特征权重$\theta_1,\theta_2,\theta_3,...,\theta_n$）

**训练线性回归模型就是设置模型参数直到模型最拟合训练集的过程**，为此我们首先需要知道怎么测量模型对训练数据拟合程度的好坏。回归模型最常见的性能指标是均方根误差（`RMSE`），因此在训练线性回归模型时，似乎及就是要找到最小化`RMSE`的$\theta$值。不过在实践中，经常用均方误差（`MSE`）最小化代替`RMSE`最小化，因为`MSE`计算简单且和`RMSE`效果相同。

在训练集`X`上，使用如下公式计算线性回归的`MSE`（$h_\theta$是假设函数）：
$$
MSE=(X,h_\theta)=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2
$$

为了得到成本函数最小的$\theta$值，有一个闭式解法（直接得出结果的数学方程），即标准方程：
$$
\theta=(X^TX)^{-1}X^Ty
$$
在这个方程中：
* $\theta$是使成本函数最小的$\theta$值
* y是包含$y^{(1)}$到$y^{(m)}$的目标值向量

我们生成一些随机的线性数据来测试这个公式：
```python
import numpy as np
X = 2 * np.random.rand(100,1)
y = 4 + 3*X +np.random.randn(100,1)
```
得到如下的线性数据集：
![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/05ad657f63d64c4294484114dc735849~tplv-k3u1fbpfcp-watermark.image?)

现在我们用标准方程来计算$\theta$。使用`NumPy`提供的线性代数模块（`np.linalg`）中的`inv()`函数来对矩阵求逆，并用`dot()`函数计算矩阵的内积：
```python
X_b = np.c_[np.ones((100,1)),X] # add x0=1 to each instance
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.t).dot(y) 
```
我们实际生成数据的函数是$y=4+3x_1+高斯噪音$，来看下公式结果：
```python
>>>theta_best
array([[4.21509616],[2.77011339]])
```
这和我们期待的$\theta_0=4$（4.21509616）、$\theta_1=3$（2.77011339）非常接近，不过由于噪音的存在使其不能完全还原为原本的函数，我们绘制出预测结果：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b80a87259f6c493fad7b5afdb3954788~tplv-k3u1fbpfcp-watermark.image?)

计算复杂度相对于想要预测的实例数量和特征数量来说都是线性的，对双倍的实例（或双倍的特征数）进行预测，大概需要双倍的时间。